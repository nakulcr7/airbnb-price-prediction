---
title: "Nonlinear Models"
author: "Nakul Camasamudram"
date: "11/18/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Nonlinear models
====================

```{r}
require(ISLR)
attach(Wage)
```


```{r}

```

Polynomials
-----------

First we will use polynomials, and focus on a single predictor age:

```{r}
fit = lm(wage ~ poly(age, 4), data = Wage)
summary(fit)
```

The 'poly()' function generates a basis of *orthogonal polynomials*.

Let's make a plot of the fitted function, along with the standard errors of the fit.

```{r fig.width=7, fig.height=6}
age.grid = seq(min(age), max(age))
preds = predict(fit, newdata = list(age = age.grid), se = TRUE)
se.bands = cbind(preds$fit + 2 * preds$se, preds$fit - 2 * preds$se)
plot(age, wage, col = "darkgrey")
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lty = 2, col = "blue")
```

There are other more direct ways of doing this in R. For example

```{r}
fita = lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)
summary(fita)
```

Here `I()` is a *wrapper* function; we need it because `age^2` means something to the formula language, while `I(age^2)` is protected.

The coefficients are different to those we got before! However, fits are the same:

```{r}
plot(fitted(fit), fitted(fita))
```

By using orthogonal polynomials in this simple way, it turns out that we can separately test for each coefficient. So, if we look at the summary again, we can see that the linear, quadratic and cubic terms are significant, but, not the quartic.

```{r}
summary(fit)
```

This only works with linear regression, and if there is a single predictor. In general, we would use `anova()` as this next example demonstrates.

```{r}
fita = lm(wage ~ education, data = Wage)
fitb = lm(wage ~ education + age, data = Wage)
fitc = lm(wage ~ education + poly(age, 2), data = Wage)
fitd = lm(wage ~ education + poly(age, 3), data = Wage)
anova(fita, fitb, fitc, fitd)
```



### Polynomial Logistic Regression
----------------------------------

Now, we fit a logistic regression model to a binary response variable, constructed from `wage`. We code the big earners (`> 250k`) as 1, else 0.

```{r}
fit = glm(I(wage > 250) ~ poly(age, 3), data = Wage, family = binomial)
summary(fit)
preds = predict(fit, newdata = list(age=age.grid), se = TRUE)
se.bands = preds$fit + cbind(fit = 0, lower = -2 * preds$se, upper = 2 * preds$se)
```

We have done the computations on the logit scale. To transform, we need to apply the inverse logit mapping.

$$ p = \frac{e^\eta}{1 + e^\eta}$$
We can do this simultaneously for all three coluns of `se.bands`:

```{r}
prob.bands = exp(se.bands) / (1 + exp(se.bands))
matplot(age.grid, prob.bands, col = "blue", lwd = c(2, 1, 1), lty = c(1, 2, 2), type = "l", ylim = c(0, 0.1))
points(jitter(age), I(wage > 250) / 10, pch = "l", cex = 0.5)
```

## Splines:
============

Splines are more flexible than polynomials, but the idea is rather similar.

Let's explore cubic splines.

```{r}
require(splines)
knots = c(25, 40, 60)
fit = lm(wage ~ bs(age, knots = knots), data = Wage)
plot(age, wage, col = "darkgrey")
lines(age.grid, predict(fit, list(age=age.grid)), col = "darkgreen", lwd = 2)
abline(v = knots, lty = 2, col = "darkgreen")
```

The smoothing spline do not require knot selection, but, it does have a smoothing parameter, which can conveniently be specified via the effective degrees of freedom of `df`.

```{r}
# Now, smoothing splines
fit = smooth.spline(age, wage, df = 16)
lines(fit, col = "red", lwd = 2)
```

Or, we can use LOO cross-validation to select the smoothing parameter for us automatically:

```{r}
fit = smooth.spline(age, wage, cv = TRUE)
lines(fit, col = "purple", lwd = 2)
fit
```


Generalized Additive Models
---------------------------

So far we have focused on fitting models with mostly single nonlinear terms.

```{r fig.width=10, fig.height=5}
require(gam)
gam1 = gam(wage ~ s(age, df = 4) + s(year, df = 4) + education, data = Wage)
par(mfrow = c(1, 3))
plot(gam1, se = TRUE)

gam2 = gam(I(wage > 250) ~ s(age, df = 4) + s(year, df = 4) + education, data = Wage, family = binomial)
plot(gam2)
```


Let's see if we need a nonlinear term for year

```{r}
gam2a = gam(I(wage > 250) ~ s(age, df = 4) + year + education, data = Wage, family = binomial)
anova(gam2a, gam2, test = "Chisq")
```

`gam` package can be used to plot models fit with `lm` and `glm` as well.

```{r}
par(mfrow = c(1, 3))
# Natural spline basis function
lm1 = lm(wage ~ ns(age, df = 4) + ns(year, df = 4) + education, data = Wage)
plot.gam(lm1, se = TRUE)
```